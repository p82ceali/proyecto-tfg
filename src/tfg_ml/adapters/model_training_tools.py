# tools/model_training_tool.py
"""
Model training tool (CrewAI BaseTool) for classical ML workflows.

Capabilities
-----------
- Performs a train/test split (stratified for classification).
- Trains one of several supported models (scikit-learn or XGBoost when available).
- Computes key metrics (classification or regression).
- Persists artifacts (serialized model + metrics JSON) to disk.

Usage
-----

Provide structured inputs via `ModelTrainingInput`. If `target` is omitted,
the last DataFrame column is used as a heuristic.
"""

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, Optional, Type, Tuple

import os
import json
import joblib
import numpy as np
import pandas as pd
from pydantic import BaseModel, Field, field_validator
from crewai.tools import BaseTool

from tfg_ml.context import CTX

dataset_path="data/dataset.csv"

def _get_features_target(df: pd.DataFrame, target: Optional[str]) -> Tuple[pd.DataFrame, pd.Series, str]:
    """
    Split a DataFrame into features (X) and target (y).

    Args:
        df: Source DataFrame.
        target: Optional target column name. If None, uses the last column.

    Returns:
        (X, y, target_name)
    """
    if target is None:
        # Heuristic: use the last column if not specified
        target = df.columns[-1]
    if target not in df.columns:
        raise ValueError(f"Target column '{target}' not found. Available: {list(df.columns)}")
    X = df.drop(columns=[target])
    y = df[target]
    return X, y, target


def _safe_mkdir(path: str) -> str:
    """Create a directory if it does not exist and return the path."""
    os.makedirs(path, exist_ok=True)
    return path


def _dump_json(path: str, data: Dict[str, Any]) -> None:
    """Persist a dictionary to JSON with UTF-8 and indentation."""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


# --------------------------------------------------------------------------------------
# Tool: ModelTrainingTool
# --------------------------------------------------------------------------------------
SUPPORTED_MODELS = [
    "logistic_regression",
    "random_forest",
    "xgboost",
    "svm",
    "linear_regression",
]


class SupportedModels(str, Enum):
    """Enumeration of supported model identifiers."""
    logistic_regression = "logistic_regression"
    random_forest = "random_forest"
    xgboost = "xgboost"
    svm = "svm"
    linear_regression = "linear_regression"


class ModelTrainingInput(BaseModel):
    """
    Structured inputs for model training.

    Notes:
        - `problem_type` must be "classification" or "regression".
        - If `target` is None, the last DataFrame column is used.
        - `artifacts_dir` controls where the trained model and metrics JSON are written.
    """
    problem_type: str = Field(..., description="Problem type: classification or regression.")
    target: Optional[str] = Field(None, description="Target column name. If None, the last DataFrame column is used.")
    model: SupportedModels = Field(
        SupportedModels.random_forest,
        description=f"Model to train. Options: {SUPPORTED_MODELS}",
    )
    test_size: float = Field(0.2, ge=0.05, le=0.5, description="Proportion for the test split.")
    random_state: int = Field(42, description="Random seed.")
    n_estimators: Optional[int] = Field(None, description="# trees (RF/XGB)")
    max_depth: Optional[int] = Field(None, description="Max depth (RF/XGB)")
    learning_rate: Optional[float] = Field(None, description="Learning rate (XGB)")
    C: Optional[float] = Field(None, description="C parameter (SVM / LogisticRegression)")
    penalty: Optional[str] = Field(None, description="Penalty for LogisticRegression (l1, l2, elasticnet, none)")
    solver: Optional[str] = Field(None, description="Solver for LogisticRegression")
    artifacts_dir: str = Field("data/artifacts", description="Directory to save model and metrics.")
    model_name: Optional[str] = Field(None, description="Base filename for the model. Autogenerated if None.")

    @field_validator("penalty")
    @classmethod
    def _normalize_penalty(cls, v: Optional[str]) -> Optional[str]:
        """Normalize 'none' (string) to None for LogisticRegression compatibility."""
        return None if (v is None or str(v).lower() == "none") else v
    
    @field_validator("n_estimators", "max_depth", "learning_rate", "C", "penalty", "solver", mode="before")
    @classmethod
    def _none_string_to_none(cls, v):
        if isinstance(v, str) and v.strip().lower() == "none":
            return None
        return v


class ModelTrainingTool(BaseTool):
    """
    Train a model for either classification or regression.

    Workflow:
        1) Split data into train/test (stratify if classification).
        2) Build model from the requested family (RandomForest, LogisticRegression, SVM,
           LinearRegression, or XGBoost if available).
        3) Fit the model and compute key metrics.
        4) Persist artifacts (model .joblib + metrics .json) under `artifacts_dir`.

    Returns:
        A concise, human-readable report including model, problem, target, split, artifact paths,
        and the computed metrics.

    Side effects:
        Writes files to `artifacts_dir`. Adds a decision entry to `CTX`.
    """
    name: str = "model_training"
    description: str = (
        "Train a scikit-learn model (or xgboost, if available) for classification or regression. "
        "Performs train/test split, computes metrics, and persists artifacts (model + metrics)."
    )
    args_schema: Type[ModelTrainingInput] = ModelTrainingInput
    dataset: Optional[pd.DataFrame] = None  # injected externally by the orchestrator/coordinator

    def _run(
        self,
        problem_type: str,
        target: Optional[str] = None,
        model: str = "random_forest",
        test_size: float = 0.2,
        random_state: int = 42,
        n_estimators: Optional[int] = None,
        max_depth: Optional[int] = None,
        learning_rate: Optional[float] = None,
        C: Optional[float] = None,
        penalty: Optional[str] = None,
        solver: Optional[str] = None,
        artifacts_dir: str = "data/artifacts",
        model_name: Optional[str] = None,
    ) -> str:
        try:
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import (
                accuracy_score,
                f1_score,
                precision_score,
                recall_score,
                roc_auc_score,
                mean_squared_error,
                r2_score,
                mean_absolute_error,
            )

            df = pd.read_csv(dataset_path)

            X, y, target_col = _get_features_target(df, target)

            # Split
            X_train, X_test, y_train, y_test = train_test_split(
                X,
                y,
                test_size=test_size,
                random_state=random_state,
                stratify=y if problem_type == "classification" else None,
            )

            # Model factory
            model_obj = None
            model_used = model

            if model == "random_forest":
                if problem_type == "classification":
                    from sklearn.ensemble import RandomForestClassifier
                    model_obj = RandomForestClassifier(
                        n_estimators=n_estimators or 200,
                        max_depth=max_depth,
                        random_state=random_state,
                        n_jobs=-1,
                    )
                else:
                    from sklearn.ensemble import RandomForestRegressor
                    model_obj = RandomForestRegressor(
                        n_estimators=n_estimators or 300,
                        max_depth=max_depth,
                        random_state=random_state,
                        n_jobs=-1,
                    )

            elif model == "logistic_regression":
                if problem_type != "classification":
                    return "LogisticRegression is only applicable to classification problems."
                from sklearn.linear_model import LogisticRegression
                model_obj = LogisticRegression(
                    C=C or 1.0,
                    penalty=penalty or "l2",
                    solver=solver or "lbfgs",
                    max_iter=200,
                )

            elif model == "svm":
                if problem_type != "classification":
                    return "SVM here is configured for classification only."
                from sklearn.svm import SVC
                model_obj = SVC(C=C or 1.0, probability=True, random_state=random_state)

            elif model == "linear_regression":
                if problem_type != "regression":
                    return "LinearRegression is only applicable to regression problems."
                from sklearn.linear_model import LinearRegression
                model_obj = LinearRegression()

            elif model == "xgboost":
                try:
                    import xgboost as xgb
                    if problem_type == "classification":
                        model_obj = xgb.XGBClassifier(
                            n_estimators=n_estimators or 300,
                            max_depth=max_depth or 6,
                            learning_rate=learning_rate or 0.1,
                            subsample=0.9,
                            colsample_bytree=0.9,
                            random_state=random_state,
                            eval_metric="logloss",
                            n_jobs=-1,
                        )
                    else:
                        model_obj = xgb.XGBRegressor(
                            n_estimators=n_estimators or 400,
                            max_depth=max_depth or 6,
                            learning_rate=learning_rate or 0.05,
                            subsample=0.9,
                            colsample_bytree=0.9,
                            random_state=random_state,
                            n_jobs=-1,
                        )
                except Exception:
                    # Fallback to RandomForest for portability
                    model_used = "random_forest"
                    if problem_type == "classification":
                        from sklearn.ensemble import RandomForestClassifier
                        model_obj = RandomForestClassifier(
                            n_estimators=n_estimators or 200,
                            max_depth=max_depth,
                            random_state=random_state,
                            n_jobs=-1,
                        )
                    else:
                        from sklearn.ensemble import RandomForestRegressor
                        model_obj = RandomForestRegressor(
                            n_estimators=n_estimators or 300,
                            max_depth=max_depth,
                            random_state=random_state,
                            n_jobs=-1,
                        )

            else:
                return f"Model '{model}' is not supported. Options: {SUPPORTED_MODELS}"

            # Train
            model_obj.fit(X_train, y_train)

            # Metrics
            metrics: Dict[str, Any] = {"problem_type": problem_type, "model": model_used, "target": target_col}
            if problem_type == "classification":
                y_pred = model_obj.predict(X_test)
                # AUC needs probabilities or decision scores
                try:
                    y_proba = model_obj.predict_proba(X_test)
                    if y_proba.shape[1] == 2:
                        auc = roc_auc_score(y_test, y_proba[:, 1])
                    else:
                        auc = roc_auc_score(y_test, y_proba, multi_class="ovr")
                except Exception:
                    auc = None
                metrics.update({
                    "accuracy": float(accuracy_score(y_test, y_pred)),
                    "precision_macro": float(precision_score(y_test, y_pred, average="macro", zero_division=0)),
                    "recall_macro": float(recall_score(y_test, y_pred, average="macro", zero_division=0)),
                    "f1_macro": float(f1_score(y_test, y_pred, average="macro", zero_division=0)),
                    "roc_auc": (float(auc) if auc is not None else None),
                })
            else:
                y_pred = model_obj.predict(X_test)
                metrics.update({
                    "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
                    "mae": float(mean_absolute_error(y_test, y_pred)),
                    "r2": float(r2_score(y_test, y_pred)),
                })

            # Persist artifacts
            out_dir = _safe_mkdir(artifacts_dir)
            base_name = model_name or f"{model_used}_{problem_type}_{target_col}"
            model_path = os.path.join(out_dir, f"{base_name}.joblib")
            metrics_path = os.path.join(out_dir, f"{base_name}_metrics.json")

            joblib.dump(model_obj, model_path)
            _dump_json(metrics_path, metrics)

            # Concise report
            header = [
                f"Model trained: {model_used}",
                f"Problem: {problem_type}",
                f"Target: {target_col}",
                f"Train/Test split: {1 - test_size:.2f}/{test_size:.2f} (random_state={random_state})",
                f"Artifacts: model -> {model_path}, metrics -> {metrics_path}",
                "KEY METRICS:",
            ]
            lines = [*header]
            for k, v in metrics.items():
                if k in {"problem_type", "model", "target"}:
                    continue
                lines.append(f"- {k}: {v}")

            report = "\n".join(lines)
            return report

        except Exception as e:
            return f"ModelTrainingTool failed: {type(e).__name__}: {e}"
